# [KAN](https://arxiv.org/abs/2404.19756)
# 启发
KAN网络结构思路来自Kolmogorov-Arnold表示定理，MLP 在节点（“神经元”）上具有固定的激活函数，而 KAN 在边（“权重”）上具有可学习的激活函数。

优点：
- 目前在可解释性上有很强的优势
- 浅层网络的表达能力有保证

不足：
- 不像MLP有很多实验验证
- 实际训练的算力要求高
# Kolmogorov-Arnold表示定理
如果f是多元连续函数，则f可以写成有限数量的单变量连续函数的两层嵌套叠加。其数学表达式就是：
$$f(x) = f(x_1,...,x_n)=\sum_{q=1}^{2n+1}\Phi_i(\sum_{p=1}^{n}\phi_{q,pj}(x_p))$$
从这里就可以看出，使用两层的网络就可以拟合任意的多元连续函数，进一步的研究发现，这个定理是不完全的，因为这个定理只是对于连续函数成立，对于一般的函数是不成立的，所以可以视为一个普适性但又不包含特殊的定理。
# 几个感兴趣的点
## 架构
<!-- image -->
<p align="center">
<img src="img\KAN.png" />
</p>

## 解释性
因为每条边可以用表示一个初等函数，节点只进行加法，所以最后的复合函数可以很容易的用解析式表示。

## 持续学习，不会发生灾难性遗忘
<p align="center">
<img src="img\KAN1.png" />
</p>

从论文的实验结果来看，KAN对输入的“窗口”内的数据进行了学习，只影响小部分“窗口”边缘的权重，而不会影响整个网络的权重，这样就不会遗忘之前的内容，这也符合人类大脑的学习模式（调整神经结构，形成新的神经连接，适应新的任务）。

# 总结
目前，因为其可解释性强，学习幻觉小，对于一些特定的任务，KAN网络可能会有很好的表现，但是因为其实际训练的算力要求高，所以目前还没有很多实际的应用，但是可以作为一个新的研究方向，对于深度学习的发展有一定的推动作用。
对于science需求精确的要求，可以很好满足。
